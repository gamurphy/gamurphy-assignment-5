{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ebt9WKHLtHiC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/train.csv')\n",
        "print(df.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZB_mO7Ulhsj",
        "outputId": "c2a0eee7-68ed-4d3d-b14a-268a1ceff3b7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 15000 entries, 0 to 14999\n",
            "Data columns (total 14 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   id               15000 non-null  int64  \n",
            " 1   CustomerId       15000 non-null  float64\n",
            " 2   Surname          15000 non-null  object \n",
            " 3   CreditScore      15000 non-null  float64\n",
            " 4   Geography        15000 non-null  object \n",
            " 5   Gender           15000 non-null  object \n",
            " 6   Age              15000 non-null  float64\n",
            " 7   Tenure           15000 non-null  float64\n",
            " 8   Balance          15000 non-null  float64\n",
            " 9   NumOfProducts    15000 non-null  float64\n",
            " 10  HasCrCard        15000 non-null  float64\n",
            " 11  IsActiveMember   15000 non-null  float64\n",
            " 12  EstimatedSalary  15000 non-null  float64\n",
            " 13  Exited           15000 non-null  float64\n",
            "dtypes: float64(10), int64(1), object(3)\n",
            "memory usage: 1.6+ MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "rqFkyL0HtPUQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca4cec5f-fa92-4a46-d823-178ce8757856"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "73ZE6xzytHiE"
      },
      "outputs": [],
      "source": [
        "# Define the KNN class\n",
        "class KNN:\n",
        "    def __init__(self, k=3, distance_metric='euclidean'):\n",
        "        self.k = k\n",
        "        self.distance_metric = distance_metric\n",
        "\n",
        "    def fit(self, features, labels):\n",
        "        self.train_data = features\n",
        "        self.train_labels = labels\n",
        "\n",
        "    def predict(self, data_points):\n",
        "      group_size = 500\n",
        "      num_samples = data_points.shape[0]\n",
        "      result_predictions = np.empty(num_samples)  # Preallocate array\n",
        "\n",
        "      for start_idx in range(0, num_samples, group_size):\n",
        "          stop_idx = min(start_idx + group_size, num_samples)\n",
        "          data_batch = data_points[start_idx:stop_idx]\n",
        "\n",
        "          computed_distances = self.calculate_distances(data_batch, self.train_data)\n",
        "          nearest_indices = np.argsort(computed_distances, axis=1)[:, :self.k]\n",
        "          nearest_labels = self.train_labels[nearest_indices]\n",
        "\n",
        "          result_predictions[start_idx:stop_idx] = np.mean(nearest_labels, axis=1)\n",
        "\n",
        "      return result_predictions\n",
        "\n",
        "    def calculate_distances(self, data1, data2):\n",
        "        if self.distance_metric == 'euclidean':\n",
        "           distances = np.sqrt(np.maximum(np.sum(np.square(data1), axis=1, keepdims=True) - 2 * np.dot(data1, data2.T) + np.sum(np.square(data2), axis=1, keepdims=True).T, 0))\n",
        "        elif self.distance_metric == 'manhattan':\n",
        "            distances = np.sum(np.abs(data1[:, np.newaxis] - data2), axis=2)\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid distance metric: {self.distance_metric}\")\n",
        "        return distances\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "svwfHyXQtHiF",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "def preprocess_data(training_file, testing_file):\n",
        "    # Load the training and testing datasets\n",
        "    train_df = pd.read_csv(training_file)\n",
        "    test_df = pd.read_csv(testing_file)\n",
        "\n",
        "    # Separate features and target from training data\n",
        "    features_train = train_df.drop(columns=['Exited', 'CustomerId', 'Surname'])\n",
        "    target_train = train_df['Exited'].values\n",
        "    features_test = test_df.drop(columns=['CustomerId', 'Surname'])\n",
        "\n",
        "    # Combine train and test features for consistent preprocessing\n",
        "    merged_data = pd.concat([features_train, features_test], axis=0, ignore_index=True)\n",
        "\n",
        "    # Identify categorical columns for one-hot encoding\n",
        "    categorical_columns = ['Geography', 'Gender', 'HasCrCard', 'IsActiveMember']\n",
        "    merged_data = pd.get_dummies(merged_data, columns=categorical_columns, dtype=float)\n",
        "\n",
        "    # Convert all columns to float\n",
        "    merged_data = merged_data.astype(float)\n",
        "\n",
        "    # Normalize features\n",
        "    feature_names = merged_data.columns.tolist()\n",
        "    for feature in feature_names:\n",
        "        feature_min = merged_data[feature].min()\n",
        "        feature_max = merged_data[feature].max()\n",
        "        range_value = feature_max - feature_min\n",
        "        if range_value > 0:\n",
        "            merged_data[feature] = (merged_data[feature] - feature_min) / range_value\n",
        "        else:\n",
        "            merged_data[feature] = 0\n",
        "\n",
        "    # Split the processed data back into training and testing sets\n",
        "    processed_train_features = merged_data.iloc[:len(features_train)].values\n",
        "    processed_test_features = merged_data.iloc[len(features_train):].values\n",
        "\n",
        "    return processed_train_features, target_train, processed_test_features\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WV_j9k-3tHiG"
      },
      "outputs": [],
      "source": [
        "def cross_validate(features, target, knn_model, num_folds=5):\n",
        "    total_samples = features.shape[0]\n",
        "    sample_indices = np.arange(total_samples)\n",
        "    np.random.shuffle(sample_indices)\n",
        "\n",
        "    unique_classes, target_indices = np.unique(target, return_inverse=True)\n",
        "    folds = [[] for _ in range(num_folds)]\n",
        "\n",
        "    for unique_class in unique_classes:\n",
        "        class_indices = sample_indices[target[sample_indices] == unique_class]\n",
        "        np.random.shuffle(class_indices)\n",
        "        class_fold_chunks = np.array_split(class_indices, num_folds)\n",
        "\n",
        "        for fold_index in range(num_folds):\n",
        "            folds[fold_index].extend(class_fold_chunks[fold_index])\n",
        "\n",
        "    area_under_curve_scores = []\n",
        "\n",
        "    for fold_index in range(num_folds):\n",
        "        validation_indices = np.array(folds[fold_index])\n",
        "        training_indices = np.setdiff1d(sample_indices, validation_indices)\n",
        "\n",
        "        X_train_split = features[training_indices]\n",
        "        y_train_split = target[training_indices]\n",
        "        X_val_split = features[validation_indices]\n",
        "        y_val_split = target[validation_indices]\n",
        "\n",
        "        knn_model.fit(X_train_split, y_train_split)\n",
        "        predictions_val = knn_model.predict(X_val_split)\n",
        "\n",
        "        auc_score = calculate_roc_auc(y_val_split, predictions_val)\n",
        "        area_under_curve_scores.append(auc_score)\n",
        "\n",
        "    return np.mean(area_under_curve_scores)\n",
        "\n",
        "def calculate_tpr_fpr(y_actual, y_probabilities, threshold_values):\n",
        "    positive_count = np.sum(y_actual == 1)\n",
        "    negative_count = np.sum(y_actual == 0)\n",
        "\n",
        "    true_positive_rates, false_positive_rates = zip(*[\n",
        "        (\n",
        "            np.sum((y_actual == 1) & (y_probabilities >= threshold)) / positive_count if positive_count > 0 else 0,\n",
        "            np.sum((y_actual == 0) & (y_probabilities >= threshold)) / negative_count if negative_count > 0 else 0\n",
        "        )\n",
        "        for threshold in threshold_values\n",
        "    ])\n",
        "\n",
        "    return np.array(true_positive_rates), np.array(false_positive_rates)\n",
        "\n",
        "def calculate_roc_auc(y_actual, y_probabilities):\n",
        "    unique_thresholds = np.sort(np.unique(y_probabilities))[::-1]\n",
        "    true_positive_rates, false_positive_rates = calculate_tpr_fpr(y_actual, y_probabilities, unique_thresholds)\n",
        "\n",
        "    # Prepend and append 0 and 1 for true and false positive rates\n",
        "    true_positive_rates = np.r_[0, true_positive_rates, 1]\n",
        "    false_positive_rates = np.r_[0, false_positive_rates, 1]\n",
        "\n",
        "    return np.trapz(true_positive_rates, false_positive_rates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "W28-GR4otHiH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70347896-b4e6-4705-9ae8-32078f24b5f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k=3, distance_metric=euclidean, cv_score=0.8351045710405733\n",
            "k=5, distance_metric=euclidean, cv_score=0.8593752033800804\n",
            "k=7, distance_metric=euclidean, cv_score=0.8725500247753744\n",
            "k=9, distance_metric=euclidean, cv_score=0.881341260660989\n",
            "k=3, distance_metric=manhattan, cv_score=0.826956612971762\n",
            "k=5, distance_metric=manhattan, cv_score=0.8594398663955497\n",
            "k=7, distance_metric=manhattan, cv_score=0.8733790407515618\n",
            "k=9, distance_metric=manhattan, cv_score=0.8793407844155899\n",
            "Best hyperparameters: k=9, distance_metric=euclidean, cv_score=0.881341260660989\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data\n",
        "X, y, X_test = preprocess_data('train.csv', 'test.csv')\n",
        "\n",
        "best_k = None\n",
        "best_metric = None\n",
        "best_cv_score = 0\n",
        "\n",
        "for distance_metric in ['euclidean', 'manhattan']:\n",
        "  for k in [3, 5, 7, 9]:\n",
        "    knn = KNN(k=k, distance_metric=distance_metric)\n",
        "    cv_score = cross_validate(X, y, knn)\n",
        "    print(f\"k={k}, distance_metric={distance_metric}, cv_score={cv_score}\")\n",
        "    if cv_score > best_cv_score:\n",
        "      best_cv_score = cv_score\n",
        "      best_k = k\n",
        "      best_metric = distance_metric\n",
        "\n",
        "print(f\"Best hyperparameters: k={best_k}, distance_metric={best_metric}, cv_score={best_cv_score}\")\n",
        "\n",
        "knn = KNN(k=best_k, distance_metric=best_metric)\n",
        "knn.fit(X, y)\n",
        "test_predictions = knn.predict(X_test)\n",
        "\n",
        "pd.DataFrame({'id': pd.read_csv('test.csv')['id'], 'Exited': test_predictions}).to_csv('submissions.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cs506",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}